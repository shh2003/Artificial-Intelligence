# 개념 정리

---

## 1. Underfitting (과소적합)

**정의:**  
모델이 데이터를 충분히 학습하지 못해 성능이 낮은 상태

**원인:**  
- 데이터 양이 부족  
- 모델이 너무 단순함  
- 학습 시간이 부족함 (epoch 작음)

**해결 방법:**  
- 데이터 추가 수집  
- 모델 복잡도 증가  
- 더 많은 학습 (epoch 증가)

---

## 2. Overfitting (과적합)

**정의:**  
모델이 훈련 데이터에 과도하게 맞춰져, 일반화 성능이 낮아지는 상태

**원인:**  
- 모델이 너무 복잡함  
- 데이터에 노이즈가 많음  
- 데이터 수에 비해 모델이 큼

**해결 방법:**  
- 모델 단순화  
- 노이즈 제거  
- 정규화 (L1/L2 Regularization)  
- 드롭아웃(Dropout), 조기 종료(Early stopping)  
- 교차검증 활용

---

## 3. 회귀 (Regression)

**정의:**  
입력 변수(x)를 이용해 연속적인 출력 변수(y)를 예측하는 모델

**예시:**  
- 면적(x) → 집값(y)

---

## 4. 상관계수 (Correlation Coefficient)

**정의:**  
x와 y의 **선형 관계의 강도와 방향**을 숫자로 나타낸 것

**값의 범위:**  
- +1: 완전한 양의 상관  
-  0: 상관 없음  
- -1: 완전한 음의 상관

> 상관관계는 인과관계를 의미하지 않음!

---

## 5. Feature Engineering (피쳐 엔지니어링)

**정의:**  
기존 데이터를 바탕으로 새로운 feature(특성)를 만드는 작업

**좋은 피쳐의 조건:**  
- 타겟값과 높은 상관관계  
- 기존 feature들과 중복이 적고 독립적인 정보 제공

**예시:**  
- 날짜 → 요일, 주말 여부, 계절 등

---

## 6. 불순도 지표

### 🔹 지니 계수 (Gini Index)
- 이진 분류 시 **데이터 혼합 정도** 측정
- 0에 가까울수록 순수한 상태

### 🔹 엔트로피 (Entropy)
- 정보의 **불확실성** 측정
- 높을수록 다양한 클래스가 섞여 있음

> 결정 트리 분할 시 사용

---

## 7. 앙상블 학습 (Ensemble Learning)

**정의:**  
여러 모델을 결합하여 더 좋은 성능을 얻는 방법

**종류:**  
- Bagging: 서로 다른 데이터를 학습한 모델을 평균/투표 → Random Forest  
- Boosting: 이전 모델의 오차를 보완해 학습 → XGBoost, AdaBoost

---

## 8. 랜덤 포레스트 (Random Forest)

- 여러 결정 트리를 **부트스트래핑**으로 훈련  
- 예측 시 평균/투표 방식 사용  
- 과적합을 줄이고 성능을 높이는 앙상블 기법

---

## 9. 부트스트래핑 (Bootstrapping)

**정의:**  
데이터셋에서 **중복을 허용**하며 샘플링하여 새로운 훈련셋을 만드는 방법

**용도:**  
- 다양한 학습 데이터 제공 → 과적합 방지  
- Bagging 기반 모델 학습

---

## 10. 교차 검증 (Cross Validation)

**정의:**  
데이터를 여러 부분으로 나눠 **훈련/검증을 반복**하며 성능을 평가

**종류:**  
- K-Fold  
- Stratified K-Fold  
- Leave-One-Out 등

---

## 11. 머신러닝 vs 딥러닝

| 항목 | 머신러닝 | 딥러닝 |
|------|----------|--------|
| 데이터 전처리 | 특징 추출 필요 | 원시 데이터를 바로 사용 |
| 모델 구조 | 단순함 | 깊고 복잡함 |
| 데이터 요구량 | 적음 | 많음 |
| 주요 도구 | Scikit-learn 등 | TensorFlow, PyTorch |
| 예시 | SVM, 랜덤포레스트 | CNN, RNN, Transformer |

**딥러닝 전처리:**  
- 원핫 인코딩  
- 넘파이 배열로 변환  
- GPU 학습 환경 구성

---


